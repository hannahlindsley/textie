{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple classifier for both text and non-text features\n",
    "I've developed a pretty large number of text classifiers, and usually do not perform rules-based feature extraction. When I went to do it for a recent project however, I discovered a fun fact: \n",
    "\n",
    "There are a hell of a lot of roadblocks.\n",
    "\n",
    "Here's a condensed version of the journey, for myself and posterity. <3 <3\n",
    "\n",
    "## Setup and imports\n",
    "\n",
    "I'm using Python 3.6.3 (if you're using 2.7 still... I'm sorry? Here's your [deathclock](https://pythonclock.org/).)\n",
    "\n",
    "* [pandas 0.21.0](https://pandas.pydata.org/pandas-docs/stable/whatsnew.html)\n",
    "* [numpy 1.13.3](https://github.com/numpy/numpy/releases/tag/v1.13.3)\n",
    "* [xgboost 0.6a2](https://pypi.python.org/pypi/xgboost/)\n",
    "* [scikit-learn 0.19.1](https://pypi.python.org/pypi/scikit-learn/0.19.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.icd.classifier import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "import re\n",
    "# from collections import Counter\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "I'll use the most basic sample data here. Keep in mind that testing with single letters like 'a', 'b', etc won't work when we go to vectorize the text because single letters are usually stopwords. An empty vector model (a matrix of all zeros) is checked for and disallowed.\n",
    "\n",
    "For that reason, our test data has full words in the textual column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['x1'] = [0, 1, 1, 0]\n",
    "data['x2'] = ['love', 'hate', 'kiss', 'kill']\n",
    "data['y'] = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input matrix (denoted X in math and machine learning, but because I pretend to follow pep8 'x' henceforth in my code) should contain both the text and non-text columns, and y is brilliantly labelled already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['x1', 'x2']].copy()\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input matrix with records and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>kiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1    x2\n",
       "0   0  love\n",
       "1   1  hate\n",
       "2   1  kiss\n",
       "3   0  kill"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output array with the right answers for each input record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "Name: y, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later on, we'll go ahead and split it into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = shuffle(x, y, random_state=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, test_size=0.5, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "Because there's kind of a lot going on in the data preparation before we can call the final ```.fit(x, y)```, I use sklearn's incredibly inconvenient Pipeline api. \n",
    "\n",
    "The overall flow of what we need to do looks something like this:\n",
    "1. pass the textual column to a vectorizer\n",
    "2. don't do anything to the non-textual column, it's already one-hot encoded and everything\n",
    "3. join both the features in a FeatureUnion\n",
    "4. make the meta-pipeline from the feature union (data-prep) to the xgboost classifier (the model itself)\n",
    "\n",
    "The steps in the pipeline (except for the end model, in our case an XGBClassifier) have to all implement ```transform(...)``` and ```fit(...)```.\n",
    "\n",
    "You'll see why I don't particularly enjoy working with pipelines, starting with all the little wrapper classes referred to as **transformations** (read \"functions\"). I come from the world of FP and have never really understood why languages don't make it easier to pass around simple functions, but I digress.\n",
    "\n",
    "### Transforming sparse matrices\n",
    "\n",
    "The initial transformation I define here is kind of a weird one. Its purpose is to transform tfidf sparse matrices into something usable downstream.\n",
    "\n",
    "This is firstly to get around [a bug in xgboost](https://github.com/dmlc/xgboost/issues/1238) in the processing of csr sparse matrices (xgboost can't do it, so I convert them to pandas experimental [SparseDataFrames](https://pandas.pydata.org/pandas-docs/stable/sparse.html#sparsedataframe). For what it's worth, if you build from source they've got it fixed; I installed with pip3, so no such luck.\n",
    "\n",
    "The second purpose of this little guy is to get around [another bug, this time in pandas](https://github.com/pandas-dev/pandas/issues/5470).\n",
    "We have to add a column of ones to the tfidf matrix (you'll have to do it anytime you work with a sparse matrix, probably). You can technically get around it by using ```.todense()```\n",
    "on another matrix or by converting it to an array, but that's **incredibly** memory-intensive. Like, intractably so for any commercial problem. \n",
    "\n",
    "Adding the column of 1s is the best way I've found to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixSparseMatrix:\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(xs):\n",
    "        # work around https://github.com/dmlc/xgboost/issues/1238#issuecomment-243872543\n",
    "        # another problem : https://github.com/pandas-dev/pandas/issues/5470\n",
    "        df = pd.SparseDataFrame(xs)\n",
    "        df[df.shape[1] + 1] = np.ones(df.shape[0])\n",
    "#         return df.to_coo()\n",
    "        return df.values\n",
    "\n",
    "    def fit(self, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming a matrix into a subset of itself\n",
    "Technically a transformer, but it would have been *so* much more convenient if we thought of these as functions... ugh.\n",
    "\n",
    "Basically, if you remember our input matrix x, it had heterogenous data types that need to be processed differently. A machine learning algorithm takes a matrix of numbers, and right now we have text - something has to convert the text into correlative numeric values (NLP to the rescue!).\n",
    "\n",
    "Before we can convert the text though, we (annoyingly) have to convert x into the subset we want (in this case, it's ```x['text']```).\n",
    "\n",
    "I took this directly from the [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/hetero_feature_union) on using a TfidfVectorizer in a larger pipeline. They call it \"ItemSelector\", but in an act of defiance I name it consistently with the rest of the pipeline. Keep in mind that any transformation needs to implement both ```fit(...)``` and ```transform(...)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subset(TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping within a subset transformation\n",
    "So for some data, all we need is to grab a subset. But unfortunately, we're joining multiple features into one pipeline. Because [sklearn's FeatureUnion does not properly handle one-dimensional arrays](https://stackoverflow.com/questions/42022487/combining-heterogenous-features-in-scikit-learn), you have to reshape 1d's into 2d's. Otherwise you'll get \n",
    "```ValueError: all the input arrays must have same number of dimensions```\n",
    "\n",
    "This happens  when we try to grab just the non-textual data from x.\n",
    "\n",
    "We need another wrapper class. Look at all these lines of code for such teeny functionality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeSubset(TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text into numbers\n",
    "I didn't write this class, so I don't get to be snarky about what it's called - but the bread and butter of NLP is the vector model - one of many ways to convert text into numbers. I instantiate a TfidfVectorizer from scikit-learn, just for funsies. \n",
    "\n",
    "It already implements ```fit(...)``` and ```transform(...)```, so it'll fit into the pipeline as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooking it up \n",
    "IT BEGINS (after way too much effort)\n",
    "\n",
    "Take the text column and vectorize it, transforming its output into a pandas SparseDataFrame to get around those bugs in the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = make_pipeline(Subset(key='x2'), vec, FixSparseMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's annoying, but afaik you have to label the transformations in sklearn (I'm making dummies for them with 't' and 'nt'). The only time you don't have to do that is when you create a\n",
    "Pipeline object using the make_pipeline constructor; I don't know of any such thing for FeatureUnion though, so we can't get around it completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = FeatureUnion([('t', text_pipeline), ('nt', ReshapeSubset(key='x1'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've finally arrive at the finished pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(all_features, xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train/test on the raw X, y as if it were one\n",
    "single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('t', Pipeline(memory=None,\n",
       "     steps=[('subset', <__main__.Subset object at 0x119070978>), ('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding...logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the results?\n",
    "Well these results are terrible, which is exactly as accurate as the data. But the pipeline works! Go find you some better data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(x_test)\n",
    "predictions = [round(value) for value in predictions]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "msg = \"Accuracy: %.2f%%\\n\" % (accuracy * 100.0)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
